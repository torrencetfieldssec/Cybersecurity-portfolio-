{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Tff-NthW-t5u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fomfIIPmnou5"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# install dependencies\n",
        "!pip install -q langchain langchain-community langchain-huggingface\n",
        "!pip install -q faiss-cpu pandas requests beautifulsoup4\n",
        "!pip install -q pymupdf4llm\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q gdeltdoc pycountry newspaper4k lxml_html_clean\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# imports and setup\n",
        "from pathlib import Path\n",
        "from uuid import uuid4\n",
        "from typing import List, Dict\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import re\n",
        "import requests\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import json\n",
        "import urllib.parse\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import hashlib\n",
        "import torch\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings, HuggingFaceEndpoint\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "from gdeltdoc import GdeltDoc, Filters\n",
        "from newspaper import Article\n",
        "import pycountry\n",
        "\n",
        "# Create dirs\n",
        "Path(\"/content/tempdata\").mkdir(exist_ok=True)\n",
        "Path(\"/content/reports\").mkdir(exist_ok=True)"
      ],
      "metadata": {
        "id": "ojSA2qqdalle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch data from APIs and preprocess"
      ],
      "metadata": {
        "id": "Qtk6ovkk-3Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ACLED data connector\n",
        "def fetch_acled_data(\n",
        "    email,\n",
        "    password,\n",
        "    country,\n",
        "    eventStartDate,\n",
        "    eventEndDate,\n",
        "    chosenEventType=\"\",\n",
        "    chosenSubEventType=\"\",\n",
        "    chosenMinFatalities=1,\n",
        "    dataFields=\"event_date|disorder_type|event_type|sub_event_type|actor1|assoc_actor_1|country|location|source|notes|fatalities\",\n",
        "    directoryPath=Path(\"/content/tempdata\")\n",
        "):\n",
        "    \"\"\"Fetch ACLED conflict event data\"\"\"\n",
        "    tokenURL = \"https://acleddata.com/oauth/token\"\n",
        "    baseURL = \"https://acleddata.com/api/acled/read\"\n",
        "\n",
        "    def get_access_token(username, password, token_url):\n",
        "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
        "        data = {'username': username, 'password': password, 'grant_type': 'password', 'client_id': 'acled'}\n",
        "        response = requests.post(token_url, headers=headers, data=data)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()['access_token']\n",
        "        raise Exception(f\"Failed to get access token: {response.status_code}\")\n",
        "\n",
        "    token = get_access_token(email, password, tokenURL)\n",
        "\n",
        "    params = [\n",
        "        \"_format=json\",\n",
        "        f\"country={requests.utils.quote(country)}\",\n",
        "        f\"event_date={eventStartDate}|{eventEndDate}\",\n",
        "        \"event_date_where=BETWEEN\",\n",
        "        \"fields=\" + dataFields\n",
        "    ]\n",
        "\n",
        "    url = baseURL + \"?\" + \"&\".join(params)\n",
        "    response = requests.get(url, headers={\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"})\n",
        "    response.raise_for_status()\n",
        "\n",
        "    rows = response.json().get(\"data\", [])\n",
        "\n",
        "    out_path = directoryPath / f\"ACLED-{country}.csv\"\n",
        "    headers_all = dataFields.split(\"|\")\n",
        "    omit = {\"source\", \"meta\", \"assoc_actor_1\", \"sub_event_type\", \"country\"}\n",
        "    headers = [h for h in headers_all if h not in omit]\n",
        "\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=headers, extrasaction=\"ignore\")\n",
        "        writer.writeheader()\n",
        "        for ev in rows:\n",
        "            writer.writerow({h: ev.get(h, \"\") for h in headers})\n",
        "\n",
        "    print(f\"ACLED: {len(rows)} events to {out_path}\")\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "Fa1pNILMDsJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GDELT data connector\n",
        "def fetch_gdelt_data(\n",
        "    termToSearch,\n",
        "    startDateTime,\n",
        "    endDateTime,\n",
        "    maxNumArticles=50\n",
        "):\n",
        "    \"\"\"Fetch GDELT news articles using newspaper4k\"\"\"\n",
        "\n",
        "    # Whitelist\n",
        "    ALLOWED_DOMAINS = [\n",
        "        # Tier 1 - Major international wire services & news\n",
        "        'reuters.com', 'apnews.com', 'bbc.com', 'bbc.co.uk',\n",
        "        'aljazeera.com', 'theguardian.com', 'dw.com', 'france24.com', 'aa.com.tr',\n",
        "\n",
        "        # Tier 2 - Major global news outlets\n",
        "        'cnn.com', 'nytimes.com', 'washingtonpost.com', 'ft.com', 'economist.com',\n",
        "\n",
        "        # Tier 3 - Regional news services\n",
        "        'channelnewsasia.com', 'scmp.com', 'thehindu.com', 'straitstimes.com',\n",
        "        'thedailystar.net', 'dawn.com', 'arabnews.com', 'jpost.com', 'dailysabah.com',\n",
        "\n",
        "        # Tier 4 - Humanitarian organizations & UN\n",
        "        'reliefweb.int', 'unhcr.org', 'unocha.org', 'unicef.org',\n",
        "        'wfp.org', 'who.int', 'icrc.org', 'msf.org',\n",
        "\n",
        "        # Tier 5 - Human rights & advocacy\n",
        "        'hrw.org', 'amnesty.org', 'refugeesinternational.org', 'rescue.org',\n",
        "\n",
        "        # Tier 6 - Independent international journalism\n",
        "        'rfa.org', 'voanews.com', 'rferl.org', 'trtworld.com',\n",
        "\n",
        "        # Tier 7 - Quality African news\n",
        "        'africanews.com', 'theeastafrican.co.ke',\n",
        "\n",
        "        # Tier 8 - Latin America\n",
        "        'elpais.com', 'mercopress.com',\n",
        "    ]\n",
        "\n",
        "    def is_domain_allowed(url):\n",
        "        \"\"\"Check if URL domain is in whitelist\"\"\"\n",
        "        url_lower = url.lower()\n",
        "        for domain in ALLOWED_DOMAINS:\n",
        "            if domain in url_lower:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def get_gdelt_urls():\n",
        "        \"\"\"Fetch article URLs from GDELT API\"\"\"\n",
        "        try:\n",
        "            gd = GdeltDoc()\n",
        "\n",
        "            # Convert datetime format from YYYYMMDDHHMMSS to YYYY-MM-DD for Filters\n",
        "            start_date = datetime.strptime(startDateTime[:8], \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
        "            end_date = datetime.strptime(endDateTime[:8], \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            f = Filters(\n",
        "                keyword=termToSearch,\n",
        "                start_date=start_date,\n",
        "                end_date=end_date,\n",
        "                num_records=maxNumArticles * 3,  # Get extra to account for filtering\n",
        "                language=\"english\"\n",
        "            )\n",
        "\n",
        "            gdelt_articles = gd.article_search(f)\n",
        "            urls = []\n",
        "\n",
        "            if gdelt_articles is not None and not gdelt_articles.empty:\n",
        "                for idx, row in gdelt_articles.iterrows():\n",
        "                    if 'url' in row and row['url'] is not None:\n",
        "                        url = row['url']\n",
        "                        if is_domain_allowed(url):\n",
        "                            urls.append(url)\n",
        "                            if len(urls) >= maxNumArticles:\n",
        "                                break\n",
        "\n",
        "            return urls\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GDELT API error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def extract_article_with_newspaper4k(url, retry=True):\n",
        "        \"\"\"Extract clean article content using newspaper4k\"\"\"\n",
        "        try:\n",
        "            article = Article(url)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "            if article.text and len(article.text) > 500:\n",
        "                return {\n",
        "                    'text': article.text,\n",
        "                    'title': article.title or 'Untitled',\n",
        "                    'publish_date': str(article.publish_date) if article.publish_date else '',\n",
        "                    'url': url\n",
        "                }\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            if retry:\n",
        "                print(f\"  Retrying: {url}\")\n",
        "                time.sleep(2)\n",
        "                return extract_article_with_newspaper4k(url, retry=False)\n",
        "            else:\n",
        "                print(f\"  Extraction failed: {str(e)[:80]}\")\n",
        "                return None\n",
        "\n",
        "    try:\n",
        "        print(\"Fetching GDELT article URLs...\")\n",
        "        urls = get_gdelt_urls()\n",
        "\n",
        "        if not urls:\n",
        "            print(\"GDELT: No articles found matching criteria\")\n",
        "            return []\n",
        "\n",
        "        print(f\"GDELT: Retrieved {len(urls)} whitelisted URLs, extracting content...\")\n",
        "\n",
        "        documents = []\n",
        "        for idx, url in enumerate(urls[:maxNumArticles], 1):\n",
        "            try:\n",
        "                print(f\"  [{idx}/{min(len(urls), maxNumArticles)}] {url[:60]}...\")\n",
        "\n",
        "                article_data = extract_article_with_newspaper4k(url)\n",
        "\n",
        "                if article_data:\n",
        "                    doc = Document(\n",
        "                        page_content=article_data['text'],\n",
        "                        metadata={\n",
        "                            'source': url,\n",
        "                            'title': article_data['title'],\n",
        "                            'publish_date': article_data['publish_date'],\n",
        "                            'data_source': 'GDELT'\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "                    print(f\"  ✓ Extracted {len(article_data['text'])} characters\")\n",
        "\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing {url}: {str(e)[:80]}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"GDELT: Successfully extracted {len(documents)} articles\")\n",
        "        return documents\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"GDELT connector failed: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "sVhj5Vr1DxnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReliefWeb data connector\n",
        "def fetch_reliefweb_data(\n",
        "    country,\n",
        "    eventStartDate,\n",
        "    eventEndDate,\n",
        "    maxReports=100,\n",
        "    appname=\"UTD-AIReport-4485\"\n",
        "):\n",
        "    \"\"\"Fetch ReliefWeb humanitarian reports\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.reliefweb.int/v2/reports\"\n",
        "\n",
        "    # Handle Russia special case\n",
        "    country_api = \"Russian Federation\" if country.lower() == \"russia\" else country\n",
        "\n",
        "    def fetch_page(limit=50, retry=True):\n",
        "        \"\"\"Fetch single page of reports with retry logic\"\"\"\n",
        "        try:\n",
        "            filters = [\n",
        "                {\"field\": \"country.name\", \"value\": country_api},\n",
        "                {\n",
        "                    \"field\": \"date.created\",\n",
        "                    \"value\": {\n",
        "                        \"from\": f\"{eventStartDate}T00:00:00+00:00\",\n",
        "                        \"to\": f\"{eventEndDate}T23:59:59+00:00\"\n",
        "                    }\n",
        "                },\n",
        "                {\"field\": \"language.name\", \"value\": \"English\"},\n",
        "                {\"field\": \"format.name\", \"value\": \"Infographic\", \"negate\": True},\n",
        "                {\"field\": \"format.name\", \"value\": \"Map\", \"negate\": True},\n",
        "                {\"field\": \"format.name\", \"value\": \"Other\", \"negate\": True},\n",
        "            ]\n",
        "\n",
        "            payload = {\n",
        "                \"profile\": \"full\",\n",
        "                \"limit\": limit,\n",
        "                \"sort\": [\"date.created:desc\"],\n",
        "                \"fields\": {\n",
        "                    \"include\": [\"title\", \"url\", \"body\", \"date\", \"theme\", \"source\",\n",
        "                               \"disaster\", \"disaster_type\", \"format\"]\n",
        "                },\n",
        "                \"filter\": {\"operator\": \"AND\", \"conditions\": filters}\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                f\"{BASE_URL}?appname={appname}\",\n",
        "                json=payload,\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                timeout=30\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "\n",
        "            return response.json().get(\"data\", [])\n",
        "\n",
        "        except Exception as e:\n",
        "            if retry:\n",
        "                print(f\"  ReliefWeb API error, retrying: {str(e)[:80]}\")\n",
        "                time.sleep(2)\n",
        "                return fetch_page(limit, retry=False)\n",
        "            else:\n",
        "                print(f\"  ReliefWeb API error: {str(e)[:80]}\")\n",
        "                return []\n",
        "\n",
        "    try:\n",
        "        print(\"Fetching ReliefWeb reports...\")\n",
        "\n",
        "        reports = fetch_page(limit=min(maxReports, 250))\n",
        "\n",
        "        if not reports:\n",
        "            print(\"ReliefWeb: No reports found for criteria\")\n",
        "            return []\n",
        "\n",
        "        print(f\"ReliefWeb: Retrieved {len(reports)} reports, processing...\")\n",
        "\n",
        "        documents = []\n",
        "        for report in reports[:maxReports]:\n",
        "            try:\n",
        "                fields = report.get('fields', {})\n",
        "                body = fields.get('body', '')\n",
        "                title = fields.get('title', 'Untitled')\n",
        "                url = fields.get('url', '')\n",
        "\n",
        "                # Only include reports with substantial content\n",
        "                if body and len(body) > 200:\n",
        "                    # Extract metadata\n",
        "                    event_date = fields.get('date', {}).get('created', '')\n",
        "\n",
        "                    source_list = fields.get('source', [])\n",
        "                    source_name = source_list[0].get('shortname', 'Unknown') if source_list else 'Unknown'\n",
        "\n",
        "                    format_list = fields.get('format', [])\n",
        "                    format_name = format_list[0].get('name', 'Unknown') if format_list else 'Unknown'\n",
        "\n",
        "                    theme_list = fields.get('theme', [])\n",
        "                    themes = [t.get('name') for t in theme_list] if theme_list else []\n",
        "\n",
        "                    disaster_list = fields.get('disaster', [])\n",
        "                    disasters = [d.get('name') for d in disaster_list] if disaster_list else []\n",
        "\n",
        "                    disaster_type_list = fields.get('disaster_type', [])\n",
        "                    disaster_type = disaster_type_list[0].get('name', '') if disaster_type_list else ''\n",
        "\n",
        "                    doc = Document(\n",
        "                        page_content=body,\n",
        "                        metadata={\n",
        "                            'source': url,\n",
        "                            'title': title,\n",
        "                            'event_date': event_date,\n",
        "                            'actor1': source_name,\n",
        "                            'format': format_name,\n",
        "                            'themes': ', '.join(themes) if themes else '',\n",
        "                            'disasters': ', '.join(disasters) if disasters else '',\n",
        "                            'disaster_type': disaster_type,\n",
        "                            'data_source': 'ReliefWeb'\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  Error processing report: {str(e)[:80]}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"ReliefWeb: Successfully processed {len(documents)} reports\")\n",
        "        return documents\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ReliefWeb connector failed: {str(e)}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "tmgzelI8D3wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Worldbank data connector\n",
        "def fetch_worldbank_data(\n",
        "    country,\n",
        "    eventStartDate,\n",
        "    directoryPath=Path(\"/content/tempdata\"),\n",
        "    apiName=\"WorldBank\"\n",
        "):\n",
        "    \"\"\"Fetch World Bank economic indicators\"\"\"\n",
        "\n",
        "    # Country code mapping\n",
        "    country_codes = {\n",
        "        \"Pakistan\": \"PAK\", \"Myanmar\": \"MMR\", \"Ukraine\": \"UKR\",\n",
        "        \"Afghanistan\": \"AFG\", \"Syria\": \"SYR\", \"Yemen\": \"YEM\",\n",
        "        \"Somalia\": \"SOM\", \"Sudan\": \"SDN\", \"Ethiopia\": \"ETH\"\n",
        "    }\n",
        "\n",
        "    country_code = country_codes.get(country, country[:3].upper())\n",
        "    year = datetime.strptime(eventStartDate, \"%Y-%m-%d\").year\n",
        "\n",
        "    # Key indicators\n",
        "    indicators = {\n",
        "        \"NY.GDP.MKTP.KD.ZG\": \"GDP growth (annual %)\",\n",
        "        \"MS.MIL.XPND.GD.ZS\": \"Military expenditure (% of GDP)\",\n",
        "        \"SP.POP.TOTL\": \"Population total\",\n",
        "        \"NY.GDP.PCAP.CD\": \"GDP per capita (current US$)\",\n",
        "        \"SI.POV.NAHC\": \"Poverty headcount ratio\"\n",
        "    }\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for code, name in indicators.items():\n",
        "        url = f\"https://api.worldbank.org/v2/country/{country_code}/indicator/{code}\"\n",
        "        params = {\"date\": f\"{year-1}:{year}\", \"format\": \"json\", \"per_page\": 100}\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params, timeout=10)\n",
        "            if response.ok and len(response.json()) > 1:\n",
        "                results = response.json()[1]\n",
        "                if results:\n",
        "                    value = results[0].get(\"value\")\n",
        "                    if value is not None:\n",
        "                        data.append({\n",
        "                            \"indicator\": name,\n",
        "                            \"value\": value,\n",
        "                            \"year\": results[0].get(\"date\")\n",
        "                        })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Save to CSV\n",
        "    directoryPath.mkdir(parents=True, exist_ok=True)\n",
        "    out_path = directoryPath / f\"{apiName}-{country}.csv\"\n",
        "\n",
        "    if data:\n",
        "        pd.DataFrame(data).to_csv(out_path, index=False)\n",
        "        print(f\"WorldBank: {len(data)} indicators → {out_path}\")\n",
        "    else:\n",
        "        # Create empty file if no data\n",
        "        pd.DataFrame(columns=[\"indicator\", \"value\", \"year\"]).to_csv(out_path, index=False)\n",
        "        print(f\"WorldBank: No data available for {country}\")\n",
        "\n",
        "    return out_path\n"
      ],
      "metadata": {
        "id": "E90bhMvuETsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG tool definitions + report generation pipeline"
      ],
      "metadata": {
        "id": "DfK2KMqj-_bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid RAG\n",
        "class SimpleLexicalRetriever:\n",
        "    \"\"\"Keyword-based retrieval\"\"\"\n",
        "    def __init__(self, docs: List[Document]):\n",
        "        self.docs = docs\n",
        "        self._content = [d.page_content.lower() for d in docs]\n",
        "\n",
        "    def get_relevant_documents(self, query: str, k: int = 6):\n",
        "        q = query.lower()\n",
        "        tokens = re.findall(r\"\\w+\", q)\n",
        "        scores = []\n",
        "        for idx, txt in enumerate(self._content):\n",
        "            score = sum(txt.count(tok) for tok in tokens)\n",
        "            scores.append((score, idx))\n",
        "        scores.sort(reverse=True, key=lambda x: x[0])\n",
        "        results = []\n",
        "        for score, idx in scores[:k]:\n",
        "            if score > 0:\n",
        "                results.append(self.docs[idx])\n",
        "        return results\n",
        "\n",
        "\n",
        "class MultiSourceRetriever:\n",
        "    \"\"\"Manages hybrid retrieval across multiple data sources\"\"\"\n",
        "\n",
        "    def __init__(self, embeddings, k_semantic=6, k_lexical=6):\n",
        "        self.embeddings = embeddings\n",
        "        self.k_semantic = k_semantic\n",
        "        self.k_lexical = k_lexical\n",
        "        self.sources = {}\n",
        "\n",
        "    def add_csv_source(self, source_name: str, csv_path: Path, text_col: str, metadata_cols: List[str]):\n",
        "        \"\"\"Add a CSV data source to the retrieval system\"\"\"\n",
        "\n",
        "        if not csv_path.exists():\n",
        "            print(f\"File not found: {csv_path}\")\n",
        "            return\n",
        "\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"  ⚠ Empty CSV: {source_name}\")\n",
        "            return\n",
        "\n",
        "        # Create documents\n",
        "        documents = []\n",
        "        for _, row in df.iterrows():\n",
        "            if pd.notna(row.get(text_col)) and str(row[text_col]).strip():\n",
        "                metadata = {\n",
        "                    \"source\": source_name,\n",
        "                    **{col: str(row.get(col, \"\")) for col in metadata_cols if col in row}\n",
        "                }\n",
        "                documents.append(Document(\n",
        "                    page_content=str(row[text_col]),\n",
        "                    metadata=metadata\n",
        "                ))\n",
        "\n",
        "        if not documents:\n",
        "            print(f\"No valid documents from {source_name}\")\n",
        "            return\n",
        "\n",
        "        # Create FAISS vector store\n",
        "        dummy_dim = len(self.embeddings.embed_query(\"test\"))\n",
        "        index = faiss.IndexFlatL2(dummy_dim)\n",
        "        vector_store = FAISS(\n",
        "            embedding_function=self.embeddings,\n",
        "            index=index,\n",
        "            docstore=InMemoryDocstore(),\n",
        "            index_to_docstore_id={},\n",
        "        )\n",
        "\n",
        "        uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "        vector_store.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "        # Store both retrievers\n",
        "        self.sources[source_name] = {\n",
        "            \"vector\": vector_store,\n",
        "            \"lexical\": SimpleLexicalRetriever(documents),\n",
        "            \"documents\": documents\n",
        "        }\n",
        "\n",
        "        print(f\"{source_name}: {len(documents)} docs indexed\")\n",
        "\n",
        "    def _hybrid_retrieve_single(self, query: str, source_name: str, k: int) -> List[Document]:\n",
        "        \"\"\"Hybrid retrieval from single source\"\"\"\n",
        "        if source_name not in self.sources:\n",
        "            return []\n",
        "\n",
        "        src = self.sources[source_name]\n",
        "\n",
        "        # Semantic retrieval\n",
        "        vector_ret = src[\"vector\"].as_retriever(search_kwargs={\"k\": self.k_semantic})\n",
        "        sem_docs = vector_ret.invoke(query)\n",
        "\n",
        "        # Lexical retrieval\n",
        "        lex_docs = src[\"lexical\"].get_relevant_documents(query, k=self.k_lexical)\n",
        "\n",
        "        # Merge & deduplicate\n",
        "        seen = set()\n",
        "        merged = []\n",
        "        for doc in sem_docs + lex_docs:\n",
        "            key = (doc.page_content[:200], doc.metadata.get(\"source\"))\n",
        "            if key not in seen:\n",
        "                seen.add(key)\n",
        "                merged.append(doc)\n",
        "\n",
        "        return merged[:k]\n",
        "\n",
        "    def retrieve_by_sources(self, query: str, k_per_source: int = 5) -> Dict[str, List[Document]]:\n",
        "        \"\"\"Retrieve from each source separately\"\"\"\n",
        "        results = {}\n",
        "        for source_name in self.sources.keys():\n",
        "            results[source_name] = self._hybrid_retrieve_single(query, source_name, k_per_source)\n",
        "        return results"
      ],
      "metadata": {
        "id": "QDoIL9zkEchX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Report Generator\n",
        "class ReportGenerator:\n",
        "    \"\"\"Generates structured humanitarian situation awareness reports\"\"\"\n",
        "\n",
        "    def __init__(self, llm, retriever: MultiSourceRetriever, worldbank_path: Path, country: str, start_date: str, end_date: str):\n",
        "        self.llm = llm\n",
        "        self.retriever = retriever\n",
        "        self.country = country\n",
        "        self.start_date = start_date\n",
        "        self.end_date = end_date\n",
        "\n",
        "        # Load WorldBank data for Trends section\n",
        "        self.worldbank_data = {}\n",
        "        if worldbank_path.exists():\n",
        "            try:\n",
        "                wb_df = pd.read_csv(worldbank_path)\n",
        "                for _, row in wb_df.iterrows():\n",
        "                    self.worldbank_data[row['indicator']] = row['value']\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    def _clean_generated_text(self, text: str, section_name: str) -> str:\n",
        "      \"\"\"Clean up generated text to remove prompt echoes and formatting issues\"\"\"\n",
        "\n",
        "      # Remove common prompt echoes\n",
        "      markers = [\n",
        "        \"System:\", \"Human:\", \"Assistant:\",\n",
        "        f\"Generate the {section_name} section now:\",\n",
        "        \"Retrieved Context:\",\n",
        "        \"Country:\", \"Date Range:\"\n",
        "      ]\n",
        "\n",
        "      lines = text.split('\\n')\n",
        "      cleaned_lines = []\n",
        "      skip_until_content = True\n",
        "\n",
        "      for line in lines:\n",
        "        # Skip lines that contain prompt markers\n",
        "        if any(marker in line for marker in markers):\n",
        "            continue\n",
        "\n",
        "        # Once we hit actual content, start including lines\n",
        "        if line.strip() and skip_until_content:\n",
        "            skip_until_content = False\n",
        "\n",
        "        if not skip_until_content:\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "      return '\\n'.join(cleaned_lines).strip()\n",
        "\n",
        "    def _generate_section(self, section_name: str, query: str, system_prompt: str, k: int = 8) -> str:\n",
        "        \"\"\"Generate a report section using RAG\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Retrieve from all sources\n",
        "            source_docs = self.retriever.retrieve_by_sources(query, k_per_source=k)\n",
        "\n",
        "            # Format context by source\n",
        "            context_parts = []\n",
        "            source_counter = {}\n",
        "\n",
        "            for source, docs in source_docs.items():\n",
        "                if docs:\n",
        "                    context_parts.append(f\"\\n=== {source.upper()} DATA ===\")\n",
        "                    source_counter[source] = 0\n",
        "\n",
        "                    for doc in docs:\n",
        "                        source_counter[source] += 1\n",
        "                        meta_str = \" | \".join([f\"{k}: {v}\" for k, v in doc.metadata.items()\n",
        "                                              if k != \"source\" and v])\n",
        "                        context_parts.append(f\"\\n[{source} source {source_counter[source]}]\")\n",
        "                        if meta_str:\n",
        "                            context_parts.append(f\"Metadata: {meta_str}\")\n",
        "                        # Limit content length\n",
        "                        content = doc.page_content[:500]\n",
        "                        if len(doc.page_content) > 500:\n",
        "                            content += \"...\"\n",
        "                        context_parts.append(content)\n",
        "\n",
        "            context = \"\\n\".join(context_parts) if context_parts else \"Limited data available for this query.\"\n",
        "\n",
        "            # Generate section\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "              (\"system\", system_prompt),\n",
        "              (\"human\", f\"\"\"Country: {self.country}\n",
        "Date Range: {self.start_date} to {self.end_date}\n",
        "\n",
        "Retrieved Context:\n",
        "{context}\n",
        "\n",
        "Generate the {section_name} section now:\"\"\")\n",
        "        ])\n",
        "\n",
        "            chain = prompt | self.llm\n",
        "            response = chain.invoke({})\n",
        "\n",
        "            # FIX: Extract only the generated content\n",
        "            if hasattr(response, 'content'):\n",
        "              generated_text = response.content\n",
        "            elif hasattr(response, 'text'):\n",
        "              generated_text = response.text\n",
        "            elif isinstance(response, str):\n",
        "              generated_text = response\n",
        "            else:\n",
        "              generated_text = str(response)\n",
        "\n",
        "            # Remove any prompt echo/repetition\n",
        "            # The model sometimes repeats the prompt, so clean it\n",
        "            generated_text = generated_text.strip()\n",
        "\n",
        "            # Remove system/human prefixes if present\n",
        "            if \"System:\" in generated_text or \"Human:\" in generated_text:\n",
        "              # Split by \"Generate the\" and take everything after\n",
        "              parts = generated_text.split(f\"Generate the {section_name} section now:\")\n",
        "              if len(parts) > 1:\n",
        "                generated_text = parts[-1].strip()\n",
        "\n",
        "            generated_text = self._clean_generated_text(generated_text, section_name)\n",
        "            return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            # Graceful degradation - return error message in section\n",
        "            error_msg = f\"[Error generating this section: {str(e)[:100]}]\\n\\n\"\n",
        "            error_msg += f\"Available sources were queried but section generation failed. \"\n",
        "            error_msg += f\"This may be due to API rate limits or network issues.\"\n",
        "            print(f\"Section '{section_name}' generation failed: {str(e)[:80]}\")\n",
        "            return error_msg\n",
        "\n",
        "    def generate_report(self) -> str:\n",
        "        \"\"\"Generate complete structured report\"\"\"\n",
        "        print(f\"\\n{'-'*20}\")\n",
        "        print(f\"Generating Report: {self.country}\")\n",
        "        print(f\"Period: {self.start_date} to {self.end_date}\")\n",
        "        print(f\"{'-'*20}\\n\")\n",
        "\n",
        "        sections = []\n",
        "\n",
        "        # Title\n",
        "        sections.append(f\"### {self.country} Generated Situation Awareness Report\")\n",
        "        sections.append(f\"### {self.start_date} till {self.end_date} ###\\n\")\n",
        "\n",
        "        # Section 1: Important Ongoing Situation\n",
        "        sections.append(\"## Important Ongoing Situation:\")\n",
        "        sections.append(self._generate_section(\n",
        "          \"Important Ongoing Situation\",\n",
        "          f\"major crisis ongoing situation humanitarian emergency {self.country}\",\n",
        "          f\"\"\"You are a humanitarian analyst writing a concise situation report.\n",
        "\n",
        "Write EXACTLY 1 paragraphs (100-150 words total) describing the most significant ongoing crisis.\n",
        "\n",
        "Requirements:\n",
        "- First Half: Describe the main event/crisis with specific dates\n",
        "- Second Half: Explain immediate impacts and humanitarian concerns\n",
        "- Use inline citations like: \"The military seized power (ACLED 1, GDELT 3)\"\n",
        "- Be factual and concise\n",
        "- DO NOT repeat the prompt or instructions in your response\n",
        "- Start directly with the content\"\"\",\n",
        "          k=6\n",
        "        ))\n",
        "\n",
        "        # Section 2: Key Recent Insights\n",
        "        sections.append(\"## Key Recent Insights:\")\n",
        "        sections.append(self._generate_section(\n",
        "          \"Key Recent Insights\",\n",
        "          f\"key developments events {self.country} conflict violence political humanitarian\",\n",
        "          f\"\"\"You are a humanitarian analyst writing a concise situation report.\n",
        "\n",
        "List EXACTLY 3-4 key insights as numbered points. Each point should be 1-2 sentences (20-30 words).\n",
        "\n",
        "Format each as:\n",
        "1. [Insight about armed conflict/violence] (ACLED 2, ReliefWeb 4)\n",
        "2. [Insight about political developments] (GDELT 1, ACLED 5)\n",
        "3. [Insight about humanitarian impacts] (ReliefWeb 3)\n",
        "4. [Insight about international response] (GDELT 6)\n",
        "\n",
        "Requirements:\n",
        "- Concise, factual statements\n",
        "- Inline citations in parentheses\n",
        "- DO NOT repeat the prompt\n",
        "- Start directly with \"1.\"\n",
        "\"\"\",\n",
        "          k=8\n",
        "        ))\n",
        "\n",
        "        # Section 3: Trends\n",
        "        wb_context = \"\"\n",
        "        if self.worldbank_data:\n",
        "          wb_context = \"\\n\\nWorld Bank Economic Indicators:\\n\"\n",
        "          for indicator, value in self.worldbank_data.items():\n",
        "            wb_context += f\"- {indicator}: {value}\\n\"\n",
        "\n",
        "        trend_prompt = f\"\"\"You are a humanitarian analyst writing a concise situation report.\n",
        "\n",
        "List EXACTLY 2-3 trends as numbered points. Each point should be 2-3 sentences (40-50 words).\n",
        "\n",
        "Cover these areas:\n",
        "1. Violence/conflict patterns with specific numbers\n",
        "2. Economic indicators and impacts\n",
        "3. Protest/civil unrest patterns\n",
        "4. Humanitarian conditions changes\n",
        "\n",
        "World Bank data available:\n",
        "{wb_context if wb_context else \"Limited economic data available\"}\n",
        "\n",
        "Requirements:\n",
        "- Use specific numbers and percentages\n",
        "- Inline citations like: \"GDP declined 12% (WorldBank data)\"\n",
        "- DO NOT repeat the prompt\n",
        "- Start directly with \"1.\"\n",
        "\"\"\"\n",
        "\n",
        "        sections.append(\"## Trends:\")\n",
        "        sections.append(self._generate_section(\n",
        "          \"Trends\",\n",
        "          f\"trends patterns changes {self.country} violence economic protests humanitarian\",\n",
        "          trend_prompt,\n",
        "          k=8\n",
        "        ))\n",
        "\n",
        "        # Section 4: Recommendations\n",
        "        sections.append(\"## Recommendation:\")\n",
        "        sections.append(self._generate_section(\n",
        "          \"Recommendations\",\n",
        "          f\"recommendations humanitarian response international aid {self.country}\",\n",
        "          f\"\"\"You are a humanitarian analyst writing a concise situation report.\n",
        "\n",
        "Write EXACTLY 1 paragraphs (100-150 words total) with actionable recommendations.\n",
        "\n",
        "Structure:\n",
        "- First Half: Immediate priorities for international actors (UN, NGOs, donors)\n",
        "- Second Half: Longer-term conflict resolution and monitoring approaches\n",
        "\n",
        "Requirements:\n",
        "- Be specific and practical\n",
        "- Use inline citations\n",
        "- DO NOT repeat the prompt\n",
        "- Start directly with the content\n",
        "\"\"\",\n",
        "          k=6\n",
        "        ))\n",
        "\n",
        "        # Combine all sections\n",
        "        report = \"\\n\".join(sections)\n",
        "\n",
        "        # Save report\n",
        "        report_path = Path(\"/content/reports\") / f\"Report-{self.country}-{self.start_date}.md\"\n",
        "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(f\"\\n{'-'*20}\")\n",
        "        print(f\"Report saved: {report_path}\")\n",
        "        print(f\"{'-'*20}\\n\")\n",
        "\n",
        "        return report\n"
      ],
      "metadata": {
        "id": "n46WfBqMFAdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline\n",
        "def generate_humanitarian_report(\n",
        "    country: str,\n",
        "    start_date: str,  # YYYY-MM-DD\n",
        "    end_date: str,    # YYYY-MM-DD\n",
        "    acled_email: str = None,\n",
        "    acled_password: str = None,\n",
        "    fetch_sources: Dict[str, bool] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Complete humanitarian report generation pipeline\n",
        "\n",
        "    Args:\n",
        "        country: Country name (e.g., \"Myanmar\", \"Ukraine\")\n",
        "        start_date: Start date YYYY-MM-DD\n",
        "        end_date: End date YYYY-MM-DD\n",
        "        acled_email: ACLED account email (optional if not fetching ACLED)\n",
        "        acled_password: ACLED account password (optional if not fetching ACLED)\n",
        "        fetch_sources: Dict controlling which sources to fetch\n",
        "            {\"acled\": True, \"gdelt\": True, \"reliefweb\": True, \"worldbank\": True}\n",
        "            Default: all True\n",
        "\n",
        "    Returns:\n",
        "        str: The generated report text\n",
        "\n",
        "    Example:\n",
        "        report = generate_humanitarian_report(\n",
        "            country=\"Myanmar\",\n",
        "            start_date=\"2021-01-01\",\n",
        "            end_date=\"2021-02-01\",\n",
        "            acled_email=userdata.get(\"ACLED_EMAIL\"),\n",
        "            acled_password=userdata.get(\"ACLED_PASSWORD\")\n",
        "        )\n",
        "    \"\"\"\n",
        "\n",
        "    # Default: fetch all sources\n",
        "    if fetch_sources is None:\n",
        "        fetch_sources = {\n",
        "            \"acled\": True,\n",
        "            \"gdelt\": True,\n",
        "            \"reliefweb\": True,\n",
        "            \"worldbank\": True\n",
        "        }\n",
        "\n",
        "    print(\"\\n\" + \"-\"*20)\n",
        "    print(\"HUMANITARIAN REPORT GENERATION PIPELINE\")\n",
        "    print(\"-\"*20)\n",
        "    print(f\"Country: {country}\")\n",
        "    print(f\"Period: {start_date} to {end_date}\")\n",
        "    print(f\"Sources: {', '.join([k.upper() for k, v in fetch_sources.items() if v])}\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    data_paths = {}\n",
        "\n",
        "    # STEP 1: Fetch Data from Sources\n",
        "    print(\"\\n[STEP 1/4] Fetching data from sources...\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    failed_sources = []\n",
        "\n",
        "    # ACLED\n",
        "    if fetch_sources.get(\"acled\", False):\n",
        "        try:\n",
        "            if not acled_email or not acled_password:\n",
        "                print(\"ACLED: Credentials missing, skipping\")\n",
        "                failed_sources.append(\"ACLED (no credentials)\")\n",
        "            else:\n",
        "                data_paths[\"acled\"] = fetch_acled_data(\n",
        "                    acled_email, acled_password, country, start_date, end_date\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"ACLED fetch failed: {str(e)[:100]}\")\n",
        "            failed_sources.append(\"ACLED\")\n",
        "\n",
        "    # GDELT\n",
        "    if fetch_sources.get(\"gdelt\", False):\n",
        "        try:\n",
        "            # Convert dates to GDELT format (YYYYMMDDHHMMSS)\n",
        "            start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "            end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "            gdelt_start = start_dt.strftime(\"%Y%m%d%H%M%S\")\n",
        "            gdelt_end = end_dt.strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "            gdelt_docs = fetch_gdelt_data(\n",
        "                termToSearch=country,\n",
        "                startDateTime=gdelt_start,\n",
        "                endDateTime=gdelt_end,\n",
        "                maxNumArticles=30\n",
        "            )\n",
        "\n",
        "            if gdelt_docs:\n",
        "                data_paths[\"gdelt\"] = gdelt_docs  # Store documents directly\n",
        "            else:\n",
        "                failed_sources.append(\"GDELT (no articles extracted)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GDELT fetch failed: {str(e)[:100]}\")\n",
        "            failed_sources.append(\"GDELT\")\n",
        "\n",
        "    # ReliefWeb\n",
        "    if fetch_sources.get(\"reliefweb\", False):\n",
        "        try:\n",
        "            reliefweb_docs = fetch_reliefweb_data(\n",
        "                country, start_date, end_date, maxReports=50\n",
        "            )\n",
        "\n",
        "            if reliefweb_docs:\n",
        "                data_paths[\"reliefweb\"] = reliefweb_docs  # Store documents directly\n",
        "            else:\n",
        "                failed_sources.append(\"ReliefWeb (no reports found)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ReliefWeb fetch failed: {str(e)[:100]}\")\n",
        "            failed_sources.append(\"ReliefWeb\")\n",
        "\n",
        "    # WorldBank\n",
        "    if fetch_sources.get(\"worldbank\", False):\n",
        "        try:\n",
        "            data_paths[\"worldbank\"] = fetch_worldbank_data(\n",
        "                country, start_date\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"WorldBank fetch failed: {str(e)[:100]}\")\n",
        "            failed_sources.append(\"WorldBank\")\n",
        "\n",
        "    # Summary of data fetching\n",
        "    print(\"\\n\" + \"-\" * 20)\n",
        "    if data_paths:\n",
        "        print(f\"Successfully fetched from: {', '.join(data_paths.keys())}\")\n",
        "    if failed_sources:\n",
        "        print(f\"Failed or skipped: {', '.join(failed_sources)}\")\n",
        "\n",
        "    if not data_paths:\n",
        "        error_msg = f\"CRITICAL: No data sources available. Cannot generate report.\\n\"\n",
        "        error_msg += f\"Failed sources: {', '.join(failed_sources)}\\n\"\n",
        "        error_msg += f\"Please check API credentials and network connection.\"\n",
        "        raise Exception(error_msg)\n",
        "\n",
        "    # STEP 2: Setup Embeddings & Retrieval\n",
        "    print(\"\\n[STEP 2/4] Setting up hybrid retrieval system...\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    try:\n",
        "        embeddings = HuggingFaceEndpointEmbeddings(\n",
        "            model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            huggingfacehub_api_token=userdata.get(\"HF_TOKEN\"),\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Failed to initialize embeddings. Check HF_TOKEN: {str(e)[:100]}\")\n",
        "\n",
        "    retriever = MultiSourceRetriever(embeddings, k_semantic=6, k_lexical=6)\n",
        "\n",
        "    indexed_sources = []\n",
        "    indexing_failures = []\n",
        "\n",
        "    # Add ACLED\n",
        "    if \"acled\" in data_paths:\n",
        "        try:\n",
        "            retriever.add_csv_source(\n",
        "                \"ACLED\",\n",
        "                data_paths[\"acled\"],\n",
        "                text_col=\"notes\",\n",
        "                metadata_cols=[\"event_date\", \"disorder_type\", \"event_type\", \"actor1\", \"location\", \"fatalities\"]\n",
        "            )\n",
        "            indexed_sources.append(\"ACLED\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to index ACLED: {str(e)[:80]}\")\n",
        "            indexing_failures.append(\"ACLED\")\n",
        "\n",
        "    # Add GDELT\n",
        "    if \"gdelt\" in data_paths and isinstance(data_paths[\"gdelt\"], list):\n",
        "        try:\n",
        "            documents = data_paths[\"gdelt\"]\n",
        "\n",
        "            if documents:\n",
        "                # Create FAISS vector store\n",
        "                dummy_dim = len(embeddings.embed_query(\"test\"))\n",
        "                index = faiss.IndexFlatL2(dummy_dim)\n",
        "                vector_store = FAISS(\n",
        "                    embedding_function=embeddings,\n",
        "                    index=index,\n",
        "                    docstore=InMemoryDocstore(),\n",
        "                    index_to_docstore_id={},\n",
        "                )\n",
        "\n",
        "                uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "                vector_store.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "                retriever.sources[\"GDELT\"] = {\n",
        "                    \"vector\": vector_store,\n",
        "                    \"lexical\": SimpleLexicalRetriever(documents),\n",
        "                    \"documents\": documents\n",
        "                }\n",
        "\n",
        "                print(f\"GDELT: {len(documents)} docs indexed\")\n",
        "                indexed_sources.append(\"GDELT\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to index GDELT: {str(e)[:80]}\")\n",
        "            indexing_failures.append(\"GDELT\")\n",
        "\n",
        "    # Add ReliefWeb\n",
        "    if \"reliefweb\" in data_paths and isinstance(data_paths[\"reliefweb\"], list):\n",
        "        try:\n",
        "            documents = data_paths[\"reliefweb\"]\n",
        "\n",
        "            if documents:\n",
        "                # Create FAISS vector store\n",
        "                dummy_dim = len(embeddings.embed_query(\"test\"))\n",
        "                index = faiss.IndexFlatL2(dummy_dim)\n",
        "                vector_store = FAISS(\n",
        "                    embedding_function=embeddings,\n",
        "                    index=index,\n",
        "                    docstore=InMemoryDocstore(),\n",
        "                    index_to_docstore_id={},\n",
        "                )\n",
        "\n",
        "                uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "                vector_store.add_documents(documents=documents, ids=uuids)\n",
        "\n",
        "                retriever.sources[\"ReliefWeb\"] = {\n",
        "                    \"vector\": vector_store,\n",
        "                    \"lexical\": SimpleLexicalRetriever(documents),\n",
        "                    \"documents\": documents\n",
        "                }\n",
        "\n",
        "                print(f\"ReliefWeb: {len(documents)} docs indexed\")\n",
        "                indexed_sources.append(\"ReliefWeb\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to index ReliefWeb: {str(e)[:80]}\")\n",
        "            indexing_failures.append(\"ReliefWeb\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 20)\n",
        "    if indexed_sources:\n",
        "        print(f\"Indexed sources: {', '.join(indexed_sources)}\")\n",
        "    if indexing_failures:\n",
        "        print(f\"Indexing failed: {', '.join(indexing_failures)}\")\n",
        "\n",
        "    if not retriever.sources:\n",
        "        raise Exception(\"CRITICAL: No data sources indexed. Cannot generate report.\")\n",
        "\n",
        "    # STEP 3: Initialize LLM\n",
        "    print(\"\\n[STEP 3/4] Initializing LLM...\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    try:\n",
        "      # Check if GPU is available\n",
        "      device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "      print(f\"Using device: {device}\")\n",
        "\n",
        "      # Load model locally\n",
        "      model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "      print(f\"Loading {model_name} (this may take a few minutes)...\")\n",
        "\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "      model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        load_in_8bit=True if device == \"cuda\" else False,  # Use 8-bit quantization on GPU\n",
        "      )\n",
        "\n",
        "      # Create pipeline\n",
        "      pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.6,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "      )\n",
        "\n",
        "      # Wrap in LangChain\n",
        "      from langchain_huggingface import HuggingFacePipeline\n",
        "      llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "      print(\"LLM loaded successfully on\", device)\n",
        "\n",
        "    except Exception as e:\n",
        "      raise Exception(f\"Failed to initialize LLM: {str(e)[:200]}\")\n",
        "\n",
        "    # STEP 4: Generate Report\n",
        "    print(\"\\n[STEP 4/4] Generating structured report...\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    worldbank_path = data_paths.get(\"worldbank\", Path(\"/content/tempdata/empty.csv\"))\n",
        "    generator = ReportGenerator(\n",
        "        llm, retriever, worldbank_path, country, start_date, end_date\n",
        "    )\n",
        "    report = generator.generate_report()\n",
        "\n",
        "    print(\"\\n\" + \"-\"*20)\n",
        "    print(\"Pipeline complete\")\n",
        "    print(\"-\"*20)\n",
        "\n",
        "    return report"
      ],
      "metadata": {
        "id": "wSFS7pS4FXxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runner"
      ],
      "metadata": {
        "id": "h2e6s_AwHWEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runner\n",
        "final_report = generate_humanitarian_report(\n",
        "    country=\"Ukraine\",\n",
        "    start_date=\"2024-02-01\",\n",
        "    end_date=\"2024-03-01\",\n",
        "    acled_email=userdata.get(\"ACLED_EMAIL\"),\n",
        "    acled_password=userdata.get(\"ACLED_PASSWORD\"),\n",
        "    fetch_sources={\n",
        "        \"acled\": True,\n",
        "        \"gdelt\": True,\n",
        "        \"reliefweb\": True,\n",
        "        \"worldbank\": True\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"-\"*20)\n",
        "print(\"Generated Report:\")\n",
        "print(\"-\"*20)\n",
        "print(final_report)"
      ],
      "metadata": {
        "id": "Ighp5FHjG4hJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
